{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 10 - Computer Vision on the Edge",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzfDLX7pqfti"
      },
      "source": [
        "# **2110443 - Computer Vision (2020/2)**\n",
        "#**Lab 10 - Computer Vision on the Edge** <br>\n",
        "In this lab, we will learn how to deploy deep learning models on edge platform (Jetson Nano) by using proper method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWNUx8t6Fmgv"
      },
      "source": [
        "# 1. Image classification model deployment (Chest X-Ray Images (Pneumonia))\n",
        "This dataset is taken from https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n",
        "![Dataset samples](https://i.imgur.com/jZqpV51.png)\n",
        "\n",
        "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
        "\n",
        "Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYBsOP6lGeUh"
      },
      "source": [
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTpeCJfUWB42"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision import models as models\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# To guarantee reproducible results \n",
        "torch.manual_seed(2)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0aR3NdyLu7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEs2u-mhdMTA"
      },
      "source": [
        "## 1.1 GPU status check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6MdQTScc_fo"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0X7YMu7HrhG"
      },
      "source": [
        "## 1.2 Download and inspect pneumonia chest-xray dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbgJfCTOQ1ev"
      },
      "source": [
        "!wget -O chest_xray.zip https://www.piclab.ai/classes/cv2020/chest_xray.zip\n",
        "!unzip -qo chest_xray.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTIYi-SbTh90"
      },
      "source": [
        "### Helper function to display image from dataset ###\n",
        "def getImageFromDataset(dataset, idx):\n",
        "  sampleImage, sampleLabel = dataset.__getitem__(idx)\n",
        "  ### Revert transformation ###\n",
        "  sampleImage = ((sampleImage.permute(1,2,0).numpy() * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406]))*255\n",
        "  sampleImage = sampleImage.astype(np.uint8)\n",
        "  sampleClassName = dataset.classes[sampleLabel]\n",
        "  return sampleImage, sampleClassName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX5nenj2Tiom"
      },
      "source": [
        "### Dataset Augmentation (https://pytorch.org/docs/stable/torchvision/transforms.html) ###\n",
        "transformTrain = transforms.Compose([        \n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=(224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])\n",
        "\n",
        "transformTest =  transforms.Compose([\n",
        "        transforms.Resize(size=(224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "### Dataloader for our dataset ###\n",
        "pneumoniaTrainDataset = ImageFolder('chest_xray/train/', transform=transformTrain)\n",
        "pneumoniaTestDataset = ImageFolder('chest_xray/test/', transform=transformTest)\n",
        "\n",
        "print('Total train set images :', len(pneumoniaTrainDataset))\n",
        "print('Total test set images :', len(pneumoniaTestDataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXt4pVMHI4M"
      },
      "source": [
        "## 1.3 Dataset visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7puG0Xn6SDxI"
      },
      "source": [
        "normalImage, normalClassName = getImageFromDataset(pneumoniaTrainDataset, 0)\n",
        "pneumoniaImage, pneumoniaClassName = getImageFromDataset(pneumoniaTrainDataset, 3000)\n",
        "\n",
        "\n",
        "_, figure = plt.subplots(1,2)\n",
        "\n",
        "figure[0].imshow(normalImage,cmap='gray')\n",
        "figure[0].title.set_text(normalClassName)\n",
        "\n",
        "figure[1].imshow(pneumoniaImage,cmap='gray')\n",
        "figure[1].title.set_text(pneumoniaClassName)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaU_BPy8S1NX"
      },
      "source": [
        "## 1.4 Construct the model from pretrained network (timm), optimizer and loss function\n",
        "\n",
        "timm documentation : https://rwightman.github.io/pytorch-image-models/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKqoSO3klZzA"
      },
      "source": [
        "pneuNet = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHm0GuydTPy7"
      },
      "source": [
        "pneuNet.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(pneuNet.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
        "\n",
        "pneumoniaTrainDatasetLoader = DataLoader(pneumoniaTrainDataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "pneumoniaTestDatasetLoader = DataLoader(pneumoniaTestDataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVDgb4Faakm5"
      },
      "source": [
        "## 1.5 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aiy7NAytaoPE"
      },
      "source": [
        "### Train and test helper function ###\n",
        "def testModel(testDatasetLoader, net):\n",
        "  net.eval()\n",
        "  correctImages = 0\n",
        "  totalImages = 0\n",
        "  allLabels = []\n",
        "  allPredicted = []\n",
        "  testingProgressbar = tqdm(enumerate(testDatasetLoader), total=len(testDatasetLoader), ncols='100%')\n",
        "  with torch.no_grad():\n",
        "    for batchIdx, batchData in testingProgressbar:\n",
        "      images, labels = batchData\n",
        "      \n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "      outputs = net(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "      correctImages += (predicted == labels).sum().item()\n",
        "      totalImages += labels.size(0)\n",
        "\n",
        "      accumulateAccuracy = round((correctImages/totalImages)*100,4)\n",
        "      testingProgressbar.set_description(\"Testing accuracy: {}\".format(accumulateAccuracy ) )\n",
        "    \n",
        "      allLabels.append(labels)\n",
        "      allPredicted.append(predicted)\n",
        "  allLabels = torch.cat(allLabels).cpu().numpy()\n",
        "  allPredicted = torch.cat(allPredicted).cpu().numpy()\n",
        "  return correctImages, totalImages, allLabels, allPredicted\n",
        "\n",
        "def trainAndTestModel(trainDatasetLoader, testDatasetLoader, net, optimizer,scheduler, criterion, trainEpoch):\n",
        "  \n",
        "  bestAccuracy = 0\n",
        "  correctImages = 0\n",
        "  totalImages = 0\n",
        "  for currentEpoch in tqdm(range(trainEpoch), desc='Overall Training Progress:', ncols='100%'):\n",
        "    trainingLoss = 0.0\n",
        "    net.train()\n",
        "    print('Epoch',str(currentEpoch+1),'/',str(trainEpoch))\n",
        "    trainingProgressbar = tqdm(enumerate(trainDatasetLoader), total=len(trainDatasetLoader), ncols='100%')\n",
        "    for batchIdx, batchData in trainingProgressbar:\n",
        "      images, labels = batchData\n",
        "      images, labels = images.cuda(), labels.cuda()\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "    \n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      correctImages += (predicted == labels).sum().item()\n",
        "      totalImages += labels.size(0)\n",
        "    \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "\n",
        "      trainingLoss += loss.item()\n",
        "      accumulateAccuracy = round((correctImages/totalImages)*100,4)\n",
        "      trainingProgressbar.set_description(\"Training accuracy: {} loss: {}\".format(accumulateAccuracy, round(loss.item(),4) ) )\n",
        "    scheduler.step(trainingLoss)\n",
        "    correctImages, totalImages, allLabels, allPredicted = testModel(testDatasetLoader, net)\n",
        "    testAccuracy = round((correctImages/totalImages)*100,2)\n",
        "\n",
        "    print('='*10)\n",
        "    \n",
        "    if testAccuracy > bestAccuracy:\n",
        "      bestAccuracy = testAccuracy\n",
        "      bestPredicted = allPredicted\n",
        "      bestNet = net\n",
        "\n",
        "  return bestAccuracy, bestPredicted, allLabels, bestNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSIG4qh1ao6r"
      },
      "source": [
        "### TODO : Train the model by using trainAndTestModel function ###\n",
        "bestAccuracy, bestPredicted, allLabels, bestNet = trainAndTestModel(pneumoniaTrainDatasetLoader, pneumoniaTestDatasetLoader, \n",
        "                                                                    pneuNet, \n",
        "                                                                    optimizer, scheduler, criterion, \n",
        "                                                                    trainEpoch=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIQB3MRxfoMv"
      },
      "source": [
        "## 1.6 Find the confusion matrix and calculate TP, TN, FP, and FN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVR-Ho82dEmw"
      },
      "source": [
        "### Confusion matrix plot helper function from https://www.kaggle.com/grfiv4/plot-a-confusion-matrix ###\n",
        "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "            \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywFnP5HgdFwF"
      },
      "source": [
        "confusionMatrix = confusion_matrix(allLabels, bestPredicted)\n",
        "plot_confusion_matrix(cm           = confusionMatrix, \n",
        "                      normalize    = False,\n",
        "                      target_names = pneumoniaTrainDataset.classes,\n",
        "                      title        = \"Pneumonia Classification Confusion Matrix\")\n",
        "tn, fp, fn, tp = confusionMatrix.ravel()\n",
        "print('TP:{} TN:{} FP:{} FN:{}'.format(tn, fp, fn, tp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZYf3dfUkk40"
      },
      "source": [
        "## 1.7 Save - Load Model in PyTorch and Model usage after training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZGQ9KDDMOva"
      },
      "source": [
        "# Save\n",
        "torch.save(pneuNet.state_dict(), '/content/drive/MyDrive/chestxray.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JixrOPSAMfV4"
      },
      "source": [
        "# Load\n",
        "weightDict = torch.load('/content/drive/MyDrive/chestxray.pth', map_location='cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCxNW2gDOMZr"
      },
      "source": [
        "pneuNet = timm.create_model('mobilenetv3_large_100', pretrained=True, num_classes=2)\n",
        "pneuNet.load_state_dict(weightDict)\n",
        "pneuNet.cuda()\n",
        "pneuNet.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fktKtjpOONA"
      },
      "source": [
        "inputImage = cv2.imread('chest_xray/val/PNEUMONIA/person1946_bacteria_4875.jpeg')\n",
        "inputImage = cv2.resize(cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB), (224,224))\n",
        "inputTensor = ((inputImage / 255) - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "\n",
        "inputTensor = torch.from_numpy(inputTensor.astype(np.float32))\n",
        "print('Before permute', inputTensor.shape)\n",
        "inputTensor = inputTensor.permute(2,0,1).unsqueeze(0)\n",
        "print('After permute and unsqueeze', inputTensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-RTQty4PTiS"
      },
      "source": [
        "with torch.no_grad():\n",
        "  inputTensor = inputTensor.cuda()\n",
        "  output = pneuNet(inputTensor)\n",
        "  _, predicted = torch.max(output, 1)\n",
        "  print('Result', output, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLeqBOroNTQn"
      },
      "source": [
        "## 1.8 Export to ONNX\n",
        "ONNX graph can be visualized by using [netron](https://netron.app)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4JiTu9uNad2"
      },
      "source": [
        "# Export the model\n",
        "torch.onnx.export(pneuNet,               # model being run\n",
        "                  inputTensor,           # model input (or a tuple for multiple inputs)\n",
        "                  \"chestxray.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=12,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                'output' : {0 : 'batch_size'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfnkra3oR3nQ"
      },
      "source": [
        "## 1.9 Run inference using ONNXRuntime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-X2vgoDR6lj"
      },
      "source": [
        "!pip install onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjDbhVKER8WT"
      },
      "source": [
        "import onnxruntime as rt\n",
        "\n",
        "sessOptions = rt.SessionOptions()\n",
        "sessOptions.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL \n",
        "chestxrayModel = rt.InferenceSession('chestxray.onnx', sessOptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqH2HowRSyIe"
      },
      "source": [
        "inputImage = cv2.imread('chest_xray/val/PNEUMONIA/person1946_bacteria_4875.jpeg')\n",
        "inputImage = cv2.resize(cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB), (224,224))\n",
        "inputTensorNp = ((inputImage / 255) - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
        "\n",
        "inputTensorNp = inputTensorNp.transpose(2,0,1)[np.newaxis].astype(np.float32)\n",
        "print('After permute and unsqueeze', inputTensorNp.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oEl9UfOS9X9"
      },
      "source": [
        "outputRT = chestxrayModel.run([], {'input': inputTensorNp})[0]\n",
        "print(outputRT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XksjkSFVe9N"
      },
      "source": [
        "# 2.Object detection model deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UNF2LpkrgQf"
      },
      "source": [
        "### Restart session (clear GPU ram)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PddbQKqQXnBf"
      },
      "source": [
        "In this lab, we will learn how to use deploy an object detection model trained by MMDetection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJI5D-WOD_69"
      },
      "source": [
        "<br>Install prerequisite libraries for MMDetection\n",
        "![mmdetection](https://raw.githubusercontent.com/open-mmlab/mmdetection/master/resources/mmdet-logo.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aeDuZWnVmr_"
      },
      "source": [
        "!pip uninstall -y torch torchvision torchtext torchaudio pycocotools # prebuilt mmdetection require PyTorch 1.8.0\n",
        "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install mmcv-full==1.3.1 -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.8.0/index.html\n",
        "!pip install mmdet==2.11.0\n",
        "!pip install mmpycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WokFWYXcVjjr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K36C1YrmWG7Y"
      },
      "source": [
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "\n",
        "%cd mmdetection\n",
        "!git checkout tags/v2.11.0\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIIZZZO8WP_f"
      },
      "source": [
        "## 2.1 Raccoon Dataset\n",
        "![Raccoon Dataset](https://i.imgur.com/cRQJ1PB.png)\n",
        "\n",
        "Dataset URL : https://github.com/datitran/raccoon_dataset <br>\n",
        "This dataset contains 196 images of raccoons and 213 bounding boxes (some images contain two raccoons). This is a single class problem, and images various size and scene condition. It's a great first dataset for getting started with object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZfTvNCEVH1"
      },
      "source": [
        "Download and extract preprocessed dataset from lab server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK1UaSHEWPKx"
      },
      "source": [
        "!wget http://piclab.ai/classes/cv2020/raccoonsDataset.zip\n",
        "!unzip -q raccoonsDataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKnPhoW1Efsb"
      },
      "source": [
        "### Dataset Exploration\n",
        "We will use pycocotools to explore this dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhNEeUwdEeNz"
      },
      "source": [
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainLabelFile='raccoons/coco_annotations.json'\n",
        "# initialize COCO api for instance annotations\n",
        "trainCOCOBinding = COCO(trainLabelFile)\n",
        "\n",
        "#display COCO categories and supercategories\n",
        "cats = trainCOCOBinding.loadCats(trainCOCOBinding.getCatIds())\n",
        "nms=[cat['name'] for cat in cats]\n",
        "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
        "\n",
        "# get all images containing given categories, select one at random\n",
        "catIds = trainCOCOBinding.getCatIds(catNms=['raccoon']);\n",
        "imgIds = trainCOCOBinding.getImgIds(catIds=catIds );\n",
        "\n",
        "randomImgId = np.random.randint(0,len(imgIds))\n",
        "sampleImageData = trainCOCOBinding.loadImgs(imgIds[randomImgId])[0]\n",
        "\n",
        "print('Image Data >>', sampleImageData)\n",
        "\n",
        "sampleImage = cv2.imread('raccoons/'+sampleImageData['file_name'])\n",
        "\n",
        "annIds = trainCOCOBinding.getAnnIds(imgIds=randomImgId, catIds=catIds, iscrowd=None)\n",
        "boxes = trainCOCOBinding.loadAnns(annIds)\n",
        "print('Box Data', boxes)\n",
        "\n",
        "for box in boxes:\n",
        "  x,y,w,h = box['bbox']\n",
        "  cv2.rectangle(sampleImage, (int(x), int(y)), (int(x+w), int(y+h)), (0,255,0), 5)\n",
        "\n",
        "sampleImage = cv2.cvtColor(sampleImage, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(sampleImage)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBBoGmwOWhzr"
      },
      "source": [
        "## 2.2 **Modify MMDetection model configuration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7KU9uFIWalL"
      },
      "source": [
        "from mmcv import Config\n",
        "\n",
        "!wget -c https://www.piclab.ai/classes/cv2020/retina_raccoon.py \\\n",
        "      -O mmdetection/configs/regnet/retina_raccoon.py\n",
        "\n",
        "modelConfig = Config.fromfile('mmdetection/configs/regnet/retina_raccoon.py') \n",
        "\n",
        "!mkdir checkpoints\n",
        "!wget -c http://download.openmmlab.com/mmdetection/v2.0/regnet/retinanet_regnetx-800MF_fpn_1x_coco/retinanet_regnetx-800MF_fpn_1x_coco_20200517_191403-f6f91d10.pth \\\n",
        "      -O checkpoints/retinanet_regnetx-800MF_fpn_1x_coco_20200517_191403-f6f91d10.pth\n",
        "\n",
        "print(f'Original Config:\\n{modelConfig.pretty_text}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNDSr4v2Wmow"
      },
      "source": [
        "from mmdet.apis import set_random_seed\n",
        "\n",
        "# Modify dataset type and path\n",
        "modelConfig.dataset_type = 'CocoDataset'\n",
        "modelConfig.data_root = './raccoons'\n",
        "modelConfig.classes = ('raccoon',)\n",
        "\n",
        "modelConfig.data.train.type = 'CocoDataset'\n",
        "modelConfig.data.train.classes = ('raccoon',)\n",
        "modelConfig.data.train.data_root = './raccoons'\n",
        "modelConfig.data.train.ann_file = 'coco_annotations.json'\n",
        "modelConfig.data.train.img_prefix = ''\n",
        "\n",
        "modelConfig.data.test.type = 'CocoDataset'\n",
        "modelConfig.data.test.classes =('raccoon',)\n",
        "modelConfig.data.test.data_root = './raccoons'\n",
        "modelConfig.data.test.ann_file = 'coco_annotations.json'\n",
        "modelConfig.data.test.img_prefix = ''\n",
        "\n",
        "modelConfig.data.val.type = 'CocoDataset'\n",
        "modelConfig.data.val.classes =('raccoon',)\n",
        "modelConfig.data.val.data_root = './raccoons'\n",
        "modelConfig.data.val.ann_file = 'coco_annotations.json'\n",
        "modelConfig.data.val.img_prefix = ''\n",
        "\n",
        "# Set up working dir to save files and logs.\n",
        "modelConfig.work_dir = './experiments'\n",
        "# use pretrained model as start point\n",
        "modelConfig.load_from = 'checkpoints/retinanet_regnetx-800MF_fpn_1x_coco_20200517_191403-f6f91d10.pth'\n",
        "\n",
        "modelConfig.optimizer.lr = 1e-3\n",
        "modelConfig.lr_config.warmup = None\n",
        "modelConfig.lr_config.policy = 'step'\n",
        "modelConfig.lr_config.step = [10,25]\n",
        "modelConfig.log_config.interval = 10\n",
        "\n",
        "# Evaluation interval\n",
        "modelConfig.evaluation.interval = 5\n",
        "# Checkpoint saving interval\n",
        "modelConfig.checkpoint_config.interval = 5\n",
        "modelConfig.runner.max_epochs = 30\n",
        "\n",
        "# Set seed thus the results are more reproducible\n",
        "modelConfig.seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "modelConfig.gpu_ids = range(1)\n",
        "\n",
        "# We can initialize the logger for training and have a look\n",
        "# at the final config used for training\n",
        "print(f'Modified Config:\\n{modelConfig.pretty_text}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9hN0Ab_Ey_9"
      },
      "source": [
        "##2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98mrSYKlWqL6"
      },
      "source": [
        "from mmdet.datasets import build_dataset\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import train_detector\n",
        "import mmcv\n",
        "import os\n",
        "\n",
        "# Build dataset\n",
        "datasets = [build_dataset(modelConfig.data.train)]\n",
        "\n",
        "# Build the detector\n",
        "model = build_detector(modelConfig.model, train_cfg=modelConfig.get('train_cfg'), test_cfg=modelConfig.get('test_cfg'))\n",
        "\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(os.path.abspath(modelConfig.work_dir))\n",
        "train_detector(model, datasets, modelConfig, distributed=False, validate=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEG8xbL5E6g7"
      },
      "source": [
        "##2.4 Inference on image!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLhoIc-5Wy4N"
      },
      "source": [
        "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
        "inputImage = mmcv.imread('raccoons/raccoon-115_jpg.rf.9723b0a68ad8ed8bdb5ccf6a210ba09b.jpg')\n",
        "\n",
        "model.cfg = modelConfig\n",
        "model.CLASSES = ('raccoons',)\n",
        "\n",
        "result = inference_detector(model, inputImage)\n",
        "show_result_pyplot(model, inputImage, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACQ4yi4E9p3"
      },
      "source": [
        "## 2.5 Export to ONNX\n",
        "List of exportable model can be read from [here](https://mmdetection.readthedocs.io/en/v2.11.0/tutorials/pytorch2onnx.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfociRfbmCOp"
      },
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0HBDx5fmREQ"
      },
      "source": [
        "!python mmdetection/tools/deployment/pytorch2onnx.py \\\n",
        "    mmdetection/configs/regnet/retina_raccoon.py \\\n",
        "    experiments/latest.pth \\\n",
        "    --output-file raccoon.onnx \\\n",
        "    --input-img raccoons/raccoon-115_jpg.rf.9723b0a68ad8ed8bdb5ccf6a210ba09b.jpg \\\n",
        "    --shape 320 320 \\\n",
        "    --mean 103.53 116.28 123.675\\\n",
        "    --std 57.375 57.12 58.395 \\\n",
        "    --dataset coco \\\n",
        "    --opset-version 11 \\\n",
        "    --show \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJl92sRtJKWC"
      },
      "source": [
        "!pip install onnx-simplifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PazrKymmJM4f"
      },
      "source": [
        "!python -m onnxsim raccoon.onnx raccoon_sim.onnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYH64VOlFGfJ"
      },
      "source": [
        "##2.6 Run inference using ONNXRuntime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owRSBFKroHE0"
      },
      "source": [
        "import onnxruntime as rt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "sessOptions = rt.SessionOptions()\n",
        "sessOptions.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL \n",
        "raccoonModel = rt.InferenceSession('raccoon_sim.onnx', sessOptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ApocJUuoQqW"
      },
      "source": [
        "inputImage = cv2.imread('raccoons/raccoon-115_jpg.rf.9723b0a68ad8ed8bdb5ccf6a210ba09b.jpg')\n",
        "ratioH, ratioW = inputImage.shape[0] / 320, inputImage.shape[1] / 320\n",
        "\n",
        "\n",
        "inputTensor = cv2.resize(inputImage, (320,320))\n",
        "inputTensor = (inputTensor - [103.53, 116.28, 123.675]) / [57.375, 57.12, 58.395]\n",
        "inputTensor = inputTensor.transpose(2,0,1)[np.newaxis].astype(np.float32)\n",
        "print('After permute and unsqueeze', inputTensor.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq4OfAVbok-P"
      },
      "source": [
        "outputRT = raccoonModel.run([], {'input': inputTensor})\n",
        "outputBoxes, outputLabels = outputRT\n",
        "print(outputBoxes.shape, outputLabels.shape)\n",
        "\n",
        "x,y,w,h,prob  = outputBoxes[0,:]\n",
        "label = outputLabels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_lU-YhMvuqN"
      },
      "source": [
        "outputImage = cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "rescaleOutputBoxes = outputBoxes * [ratioW, ratioH, ratioW, ratioH, 1]\n",
        "for boxData in zip(rescaleOutputBoxes, outputLabels):\n",
        "\n",
        "  prob  = boxData[0][4]\n",
        "\n",
        "  if prob > 0.5:\n",
        "    x1 = int(boxData[0][0])\n",
        "    y1 = int(boxData[0][1])\n",
        "    x2 = int(boxData[0][2])\n",
        "    y2 = int(boxData[0][3])\n",
        "    label = boxData[1]\n",
        "    cv2.rectangle(outputImage, (x1,y1), (x2,y2), (0,255,0), 3)\n",
        "  \n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(outputImage)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlzV0fAZwDAF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}