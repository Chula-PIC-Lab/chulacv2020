{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2110443 - Computer Vision (2020/2)\n",
    "## Lab 3 - Edge Processing\n",
    "In this lab, we will learn to utilize edge information to extract useful \"things\" from images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import OpenCV, Numpy and Matplotlib as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import random as rng\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import urllib.request\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use <a href=\"https://docs.opencv.org/3.4.1/d4/da8/group__imgcodecs.html#ga288b8b3da0892bd651fce07b3bbd3a56\">imread</a> function to read image from file. Since edge detection functions <b>require</b> a single channel input image, we must convert the input image into grayscale by using <a href=\"https://docs.opencv.org/3.4.1/d7/d1b/group__imgproc__misc.html#ga397ae87e1288a81d2363b61574eb8cab\">cvtColor</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImage = cv2.imread(\"assets/Lab3-rbc.jpg\")\n",
    "print('inputImage dimensions', inputImage.shape)\n",
    "inputImageGray = cv2.cvtColor(inputImage,cv2.COLOR_BGR2GRAY)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(inputImageGray,cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge detection is one of the fundamental operations in image processing and computer vision, including object detectors and image segmentation algorithms. It helps us reduce the amount of data  to process while still maintains the structural/shape representation of the image. OpenCV already implemented some famous edge detector. In this section, we will apply well-known edge detector on the provided image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Sobel Edge Detector\n",
    "Sobel edge detector is a gradient based method based on the first order derivatives. It separately calculates the first derivatives for the X and Y axes. You can use it by calling <a href=\"https://docs.opencv.org/3.4.2/d4/d86/group__imgproc__filter.html#gacea54f142e81b6758cb6f375ce782c8d\">cv2.Sobel</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelXOutput = cv2.Sobel(inputImageGray,cv2.CV_8U,1,0,ksize=3)\n",
    "sobelYOutput = cv2.Sobel(inputImageGray,cv2.CV_8U,0,1,ksize=3)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Sobel Output in X-Direction')\n",
    "plt.imshow(sobelXOutput, cmap='gray')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Sobel Output in Y-Direction')\n",
    "plt.imshow(sobelYOutput, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Remark</b>\n",
    "There is a slight problem with both outputs because of their data type. Since black-to-white transition is taken as positive slope (positive value) while white-to-black is taken as a negative slope (negative value), uint8 data type is only capable for positive value. When you convert data to np.uint8, all negative slopes are considered as zero and some edges will be missed. To overcome this problem we will apply absolute operator before convert into np.uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelXOutput_64F = cv2.Sobel(inputImageGray,cv2.CV_64F,1,0,ksize=3)\n",
    "print(sobelXOutput_64F.dtype)\n",
    "sobelXOutput_64F = np.uint8(np.absolute(sobelXOutput_64F))\n",
    "sobelYOutput_64F = cv2.Sobel(inputImageGray,cv2.CV_64F,0,1,ksize=3)\n",
    "sobelYOutput_64F = np.uint8(np.absolute(sobelYOutput_64F))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Sobel Output in X-Direction')\n",
    "plt.imshow(sobelXOutput_64F, cmap='gray')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Sobel Output in Y-Direction')\n",
    "plt.imshow(sobelYOutput_64F, cmap='gray')\n",
    "plt.show()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Sobel Output in Both-Direction')\n",
    "plt.imshow(sobelXOutput_64F + sobelYOutput_64F, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Robert Edge Detector\n",
    "Use the knowledge from lecture section ðŸ¤” and <b>your own</b> myFilter2D function from Lab 2 - Assignment3 to built Robert edge detector! Compare the result with original OpenCV <a href=\"https://docs.opencv.org/3.4.2/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04\">cv2.filter2D</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your filter2D function! ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILL HERE ####\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Laplacian\n",
    "Laplacian filter is an edge detector used to compute the second derivatives of an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laplacianKernel_1 = np.array([[0,-1,0], [-1,4,-1], [0,-1,0]])\n",
    "laplacianKernel_2 = np.array([[-1,-1,-1], [-1,8,-1], [-1,-1,-1]])\n",
    "\n",
    "inputImageGray = cv2.cvtColor(inputImage, cv2.COLOR_BGR2GRAY)\n",
    "laplacianOutput_1 = cv2.filter2D(inputImageGray, -1, laplacianKernel_1)\n",
    "laplacianOutput_2 = cv2.filter2D(inputImageGray, -1, laplacianKernel_2)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputImageGray, cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(laplacianOutput_1, cmap='gray')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(laplacianOutput_2, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Image Sharpening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpenKernel_1 = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n",
    "sharpenKernel_2 = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "\n",
    "inputImageGray = cv2.cvtColor(inputImage, cv2.COLOR_BGR2GRAY)\n",
    "sharpenOutput_1 = cv2.filter2D(inputImageGray, -1, sharpenKernel_1)\n",
    "sharpenOutput_2 = cv2.filter2D(inputImageGray, -1, sharpenKernel_2)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(inputImageGray, cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sharpenOutput_1, cmap='gray')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sharpenOutput_2, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Canny Edge Detector\n",
    "Canny edge detector is a popular edge detection algorithm. It is a multi-stages algorithm based on Sobel edge detector and is considered to give a better result. You can read <a href=\"https://docs.opencv.org/3.4.2/da/d22/tutorial_py_canny.html\">here</a> for short summary of this algorithm. You can Canny edge detector by calling <a href=\"https://docs.opencv.org/3.4.2/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de\">cv2.Canny</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ipywidgets and interact function to see the result of Canny edge detctor by yourself.\n",
    "#### FILL HERE ####\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Structured Edge Detector\n",
    "[Bonus] The gradient based edge detectors are typically susceptible to noise in the image and it is difficult to overcome this problem by using traditional pixel features. Instead of finding suitable rule-based methods for each task, a learning based edge detector named \"Structured Edge\" was proposed by Microsoft researcher team in International Conference on Computer Vision 2013 (ICCV2013). This algorithm use random forest to give each pixel edge intensity value based on locality feature. You can read their paper <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2013/12/DollarICCV13edges.pdf\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab edge model file from opencv_contrib repository\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/opencv/opencv_extra/master/testdata/cv/ximgproc/model.yml.gz', 'assets/structuredEdgeModel.yml.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured Edge Detector requires input to be in RGB order\n",
    "structuredEdgeDetector = cv2.ximgproc.createStructuredEdgeDetection('assets/structuredEdgeModel.yml.gz')\n",
    "inputImageRGB = cv2.cvtColor(inputImage, cv2.COLOR_BGR2RGB)\n",
    "structuredEdgeOutput = np.uint8(structuredEdgeDetector.detectEdges(np.float32(inputImageRGB) / 255.0)*255)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(structuredEdgeOutput, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Holistically-Nested Edge Detection (HED)\n",
    "[Bonus] As mentioned in previous method, the pioneer gradient based methods are typically susceptible to noise in the image difficult to overcome this problem by using traditional pixel features. Holistically-Nested Edge Detection (HED) was proposed in International Conference on Computer Vision 2015 (ICCV2015) , attempts to address the limitations of the feature based edge detector through an fully convolutional neural network structure. You can read their paper <a href=\"https://arxiv.org/abs/1504.06375\">here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab HED model file from author repository\n",
    "urllib.request.urlretrieve('http://vcl.ucsd.edu/hed/hed_pretrained_bsds.caffemodel', 'assets/hed_pretrained_bsds.caffemodel')\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/s9xie/hed/master/examples/hed/deploy.prototxt', 'assets/deploy.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object):\n",
    "    def __init__(self, params, blobs):\n",
    "        self.xstart = 0\n",
    "        self.xend = 0\n",
    "        self.ystart = 0\n",
    "        self.yend = 0\n",
    "        \n",
    "    def getMemoryShapes(self, inputs):\n",
    "        inputShape, targetShape = inputs[0], inputs[1]\n",
    "        batchSize, numChannels = inputShape[0], inputShape[1]\n",
    "        height, width = targetShape[2], targetShape[3]\n",
    "\n",
    "        self.ystart = (inputShape[2] - targetShape[2]) // 2\n",
    "        self.xstart = (inputShape[3] - targetShape[3]) // 2\n",
    "        self.yend = self.ystart + height\n",
    "        self.xend = self.xstart + width\n",
    "\n",
    "        return [[batchSize, numChannels, height, width]]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return [inputs[0][:,:,self.ystart:self.yend,self.xstart:self.xend]]\n",
    "\n",
    "cv2.dnn_registerLayer('Crop', CropLayer)\n",
    "\n",
    "hedNet = cv2.dnn.readNet('assets/hed_pretrained_bsds.caffemodel', 'assets/deploy.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputTensor = cv2.dnn.blobFromImage(inputImage, scalefactor=1.0, size=inputImage.shape[0:2], mean=(104.00698793, 116.66876762, 122.67891434), swapRB=True, crop=False)\n",
    "hedNet.setInput(inputTensor)\n",
    "hedNetOutput = cv2.resize(hedNet.forward().squeeze(), inputImage.shape[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(hedNetOutput, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the objects (Red Blood Cells) using edge information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using both X-Y Sobel edge output and apply simple threshold to keep only strong edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobelXYOutput = sobelXOutput_64F + sobelYOutput_64F\n",
    "sobelXYOutput = np.uint8(sobelXYOutput > 60)\n",
    "plt.imshow(sobelXYOutput, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize each strong edge pixel into contours by using <a href=\"https://docs.opencv.org/4.5.1/d3/dc0/group__imgproc__shape.html#gae4156f04053c44f886e387cff0ef6e08\">cv2.findContour</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbcContours, _ = cv2.findContours(sobelXYOutput, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorize each contour by using <a href=\"https://docs.opencv.org/3.4.2/d6/d6e/group__imgproc__draw.html#ga746c0625f1781f1ffc9056259103edbc\">cv2.drawContour</a>. You will see that there are some <b>noise</b> contours appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbcContourImage = inputImage.copy()\n",
    "for rcbContourIdx in range(len(rbcContours)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    # Calculates the bounding rectangle of a contour\n",
    "    x, y, w, h = cv2.boundingRect(rbcContours[rcbContourIdx])\n",
    "    cv2.drawContours(rbcContourImage, rbcContours, rcbContourIdx, color, 2)\n",
    "    cv2.rectangle(rbcContourImage,(x,y),(x+w,y+h),color,3)\n",
    "rbcContourImage = cv2.cvtColor(rbcContourImage,cv2.COLOR_BGR2RGB)\n",
    "print('Total Red Blood Cell:',len(rbcContours))\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rbcContourImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Improved Simple RBC Detector\n",
    "Is there anyway to improve the rcb detection quality? Why you do that? You can use the knowledge from previous and this lecture to answer this question! Implement your own modifiation in the following block and display the output for <b>both Lab3-rbc.jpg and Lab3-rbc2.jpg</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FILL HERE ####\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Card Detector\n",
    "Assume that we got a requirement from Chula Office of Registrar to build an automate student id card face verification system (validating an identity based on the real face and face image on card). Your team's task is to implement an automatic algorithm to <b>extract</b> student faces from their id card and collect their card into image file. It should be note that the input images can be in <b>arbitrary viewpoint</b> so you must apply homography to correct perspective distortion. You should applied the algorithm on your student id card. <b>Don't forget to censor your sensitive information.</b>\n",
    "\n",
    "<b>Guidance</b>\n",
    "1. Apply your choice edge detector\n",
    "2. Categorize each edge pixel into contours by using cv2.findContours\n",
    "3. Pick the largest contour and find its hull using <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#ga014b28e56cb8854c0de4a211cb2be656\">cv2.convexHull</a>\n",
    "4. Approximate contour into 4 points polygon to find its corners using <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#ga0012a5fdaea70b8a9970165d98722b4c\">cv2.approxPolyDP</a>\n",
    "5. Correct the perspective distortion by using cv2.getPerspectiveTransform / cv2.warpPerspective\n",
    "6. Save the output warped id card image and face into file using cv2.imwrite \n",
    "\n",
    "<b>Hint</b>\n",
    "- Too big image sometimes gives a poor edge detection result.\n",
    "- This <a href=\"https://docs.opencv.org/3.4.2/dd/d49/tutorial_py_contour_features.html\">link</a> helps a lot for 3-4\n",
    "- The points order for cv2.getPerspectiveTransform must be consistent between source and destination.\n",
    "- You can use Chula student id card face mask is given in <b>'assets/Lab3-FaceMask.bmp'</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputImage = cv2.imread('assets/Lab3-TestCard1.jpg')\n",
    "inputImage = cv2.resize(inputImage,None,fx=0.3,fy=0.3)\n",
    "inputImageGray = cv2.cvtColor(inputImage,cv2.COLOR_BGR2GRAY)\n",
    "maskImage = cv2.imread('assets/Lab3-FaceMask.bmp')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(inputImageGray,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
