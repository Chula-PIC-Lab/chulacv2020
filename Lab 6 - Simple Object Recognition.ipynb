{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2110443 - Computer Vision (2020/2)\n",
    "## Lab 6 - Simple Object Recognition\n",
    "In this lab, we will learn to use useful  handcrafted features to recognize objects in the provied images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Collaboration is encouraged in this course.</b> You must turn in <b>your own write ups</b> of all problems. If you collaborate with others, you must put the names and ids of the students you worked with in below block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaboration List:\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import random as rng\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML,clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maker Based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to apply a classic segmentation algorithm named <b>watershed</b>. This algorithm can be used to detect and extract objects in images that are <b>touching and/or overlapping</b> (like RBC in previous Lab!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to build an application to count coins value from the following image. First of all, we need to segment each coins in to a individual connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleCoinImage = cv2.imread(\"assets/Lab6-SampleCoins.jpg\")\n",
    "sampleCoinGray = cv2.cvtColor(sampleCoinImage,cv2.COLOR_BGR2GRAY)\n",
    "tempImage = cv2.cvtColor(sampleCoinImage,cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Thai Coins\")\n",
    "plt.imshow(tempImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks try to use edge information from previous lecture to seperate coins image into individual coin.\n",
    "Standard Sobel operator is used to extract edge information and only strong edge pixels are kept based on defined threshold. Try to adjust the edge threshold value to seperate each coin from others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSobelEdge(thresholdEdgeVal):\n",
    "    sampleCoinGrayBlur = cv2.blur(sampleCoinGray,(15,15))\n",
    "    sobelX = np.uint8(np.absolute(cv2.Sobel(sampleCoinGrayBlur,cv2.CV_64F,1,0,ksize=3)))\n",
    "    sobelY = np.uint8(np.absolute(cv2.Sobel(sampleCoinGrayBlur,cv2.CV_64F,0,1,ksize=3)))\n",
    "    sobelXY = (sobelX + sobelY) > thresholdEdgeVal\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Edge Image\")\n",
    "    plt.imshow(sobelXY,cmap='gray')\n",
    "    plt.show()\n",
    "interact(extractSobelEdge, thresholdEdgeVal=widgets.IntSlider(min=0,max=255,step=1,value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above threshold edge map, even we selects a precise edge threshold, we cannot segment connected coins into individual one. In order to use watershed algorithm, markers muse be place at the corresponded location of the objects in our image. The markers can be either manual define or calculate from various image processing techniques. We will start from using <a href=\"https://docs.opencv.org/3.4.2/d7/d4d/tutorial_py_thresholding.html\">automatic Otsu thesholding</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleCoinGrayBlur = cv2.medianBlur(sampleCoinGray, 13)\n",
    "_,thresholdCoinImage = cv2.threshold(sampleCoinGrayBlur,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "plt.imshow(thresholdCoinImage, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image gray levels can be visualized as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sampleCoinGray.flatten())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(sampleCoinGrayBlur <165, cmap='gray') # we can manually select a proper theshold value to sperate between background and foreground\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga8a0b7fdfcb7a13dde018988ba3a43042\">distance transform</a> to the thresholding output. Distance transform calculates the approximate or precise distance from every binary image pixel to the nearest zero pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceTransformOutput = cv2.distanceTransform(thresholdCoinImage,cv2.DIST_L2, 3)\n",
    "plt.imshow(distanceTransformOutput,cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick only pixel which is greater than 45% of max distance as potential coin markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDistance = np.max(distanceTransformOutput)\n",
    "print('Max Distance:',maxDistance)\n",
    "roughMarkerPixel = np.uint8(distanceTransformOutput > 0.45 * maxDistance)\n",
    "backgroundMask = np.invert(roughMarkerPixel)\n",
    "plt.title('Potential Coins Location')\n",
    "plt.imshow(roughMarkerPixel, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Background Pixel')\n",
    "plt.imshow(backgroundMask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling each connected component with its unique label number by using <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#gaedef8c7340499ca391d459122e51bef5\">cv2.connectedComponents</a>.\n",
    "\n",
    "Computes the connected components labeled image of boolean image\n",
    "image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0 represents the background label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, seed = cv2.connectedComponents(roughMarkerPixel)\n",
    "print('Min Label:',np.min(seed),'Max Label:',np.max(seed))\n",
    "plt.imshow(seed,cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1\">watershed algorithm</a> using created marker and input coins image. Is there any missing coin? Why? Fix the bug and state your instruction in below block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputImage = sampleCoinImage.copy()\n",
    "borderImage = sampleCoinImage.copy()\n",
    "outputMarkers = cv2.watershed(outputImage,seed) ### Warning seed will get modified here!!\n",
    "# -1 is border pixel  \n",
    "print('Min Label:',np.min(outputMarkers),'Max Label:',np.max(outputMarkers))\n",
    "\n",
    "# 1 is background \n",
    "for outputMarkerIdx in range(1,np.max(outputMarkers+1)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    outputImage[np.where(outputMarkers == outputMarkerIdx)] = color\n",
    "outputImage = cv2.cvtColor(outputImage,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Segmentation Result')\n",
    "plt.imshow(outputImage)\n",
    "plt.show()\n",
    "\n",
    "borderImage[np.where(outputMarkers == -1)] = (0,255,0)\n",
    "borderImage = cv2.cvtColor(borderImage,cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Segmentation Boundary')\n",
    "plt.imshow(borderImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crop out individual coin\n",
    "firstCointMask = outputMarkers == 1\n",
    "plt.imshow(firstCointMask)\n",
    "plt.show()\n",
    "\n",
    "contour, _ = cv2.findContours(firstCointMask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "x,y,w,h = cv2.boundingRect(contour[0])\n",
    "\n",
    "cropCoinBGR = sampleCoinImage[y:y+h,x:x+w,:]\n",
    "cropCoinRGB = cv2.cvtColor(cropCoinBGR, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(cropCoinRGB)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to find the missing coin?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1 - Coin Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above section, we can gracefully segment overlpping coins! Your today task is to implement a program to count total coin value from test images provided in assets folder. Please include two of your own test images by using smartphone camera. The images should show robustness of your designed alogorithm such as overlapping coins, tilt camera angle or shadow handling. Don't forget to show your work in step in below block. <br>(Optional) You will get extra points if you can use <b>same parameters</b> for all test images. :)\n",
    "\n",
    "<b>Remark : </b>\n",
    "- There are four test images in assets folder. Please hand in the recognition result in image files. \n",
    "- You should not count outlier object (key)!!!\n",
    "\n",
    "<b>Basic Guidance:<b>\n",
    "1. Segment each coin into individual connected component and find the bounding box which can enclose those connect components by <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#gacb413ddce8e48ff3ca61ed7cf626a366\">cv2.boundingRect</a>\n",
    "2. For each component, extract <b>useful features</b> (you have to design this by youself).\n",
    "3. Classify those components into Thai Coin classes.  \n",
    "\n",
    "<b>Hints:</b>\n",
    "- How do to discard noise/fill small hole from segmentation mask output? (Previous Lab?)\n",
    "- Smoothing input image before apply thresholding by using <a href=\"https://docs.opencv.org/4.5.1/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37\">cv2.blur</a>, <a href=\"https://docs.opencv.org/4.5.1/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9\">cv2.medianFilter</a> or <a href=\"https://docs.opencv.org/4.5.1/d4/d86/group__imgproc__filter.html#ga9fabdce9543bd602445f5db3827e4cc0\">cv2.pyrMeanShiftFiltering</a> might be able to improve threshold result.\n",
    "- From the beginning of this class, you had learned many potential image features, such as edge, color, contour and shape. Use them wisely.\n",
    "- Internet is your friend. You can search for relavent research papers and use their algorithm, but you must <b>give proper credits</b> by citing them in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Describe how your algorithm work here (Thai or English). You can provide any visualization if you want.\n",
    "'''\n",
    "1. ..\n",
    "2. ..\n",
    "..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
